import os
import sys
import json
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ inkeep_core
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from inkeep_core.extractor import ConfigExtractor
from inkeep_core.registry import SiteRegistry

def check_site(url):
    """æ£€æµ‹å•ä¸ªç«™ç‚¹æ˜¯å¦æ¥å…¥ Inkeep"""
    extractor = ConfigExtractor()
    # å°è¯•æ ¹ç›®å½•å’Œ /docs
    paths = ["", "/docs", "/introduction", "/home"]
    
    parsed = urlparse(url)
    base = f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme else f"https://{url.strip('/')}"
    
    for path in paths:
        target = base + path
        # print(f"  [Checking] {target}")
        config = extractor.scan(target)
        if config:
            return {"url": url, "detected_url": target, "found": True, "config": config}
            
    return {"url": url, "found": False}

def main():
    parser = argparse.ArgumentParser(description="Batch Inkeep Detector")
    parser.add_argument("input", help="File with list of URLs/domains")
    parser.add_argument("--output", default="scanner/scan_results.json", help="Output JSON file")
    parser.add_argument("--threads", type=int, default=10, help="Max threads")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"Error: File {args.input} not found")
        return

    with open(args.input, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸš€ Starting scan for {len(urls)} sites using {args.threads} threads...")
    
    results = []
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        future_to_url = {executor.submit(check_site, url): url for url in urls}
        
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                res = future.result()
                if res['found']:
                    print(f"âœ… FOUND: {url} -> {res['detected_url']}")
                    results.append(res)
                else:
                    print(f"âšª Not found: {url}")
            except Exception as exc:
                print(f"âŒ Error scanning {url}: {exc}")

    # ä¿å­˜ç»“æœ
    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)
        
    print(f"\nğŸ“Š Scan finished. Found {len(results)} Inkeep sites.")
    print(f"Results saved to {args.output}")

if __name__ == "__main__":
    main()
